{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import argparse"
   ],
   "id": "e193efb09e5926ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DF = pd.read_csv(\"C:\\\\Users\\TheSh\\Documents\\Programming_in_Python_Class\\PyCharmProjects\\ML_for_KRAS_G12D_Inhibitors\\Raw Files\\merged_features_IC50_g12d.csv\")\n",
    "\n",
    "print(len(DF))\n",
    "#DF[\"IC50 (nM)\"] = DF['IC50 (nM)'].drop_duplicates()\n",
    "\n",
    "\n",
    "#DF = DF.dropna()\n",
    "#print(len(DF))"
   ],
   "id": "9d0139bf12d94e90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def pIC50(input):\n",
    "    pIC50 = []\n",
    "\n",
    "    input[\"IC50 (nM)\"] = pd.to_numeric(input[\"IC50 (nM)\"],errors='coerce')\n",
    "\n",
    "    for i in input[\"IC50 (nM)\"]:\n",
    "        molar = i*(10**-9) # Converts nM to M\n",
    "        pIC50.append(-np.log10(molar))\n",
    "\n",
    "    input['IC50 (nM)'] = pIC50\n",
    "    x = input[\"IC50 (nM)\"]\n",
    "\n",
    "    return x"
   ],
   "id": "16adf740cd48475e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter and sample data before splitting\n",
    "DF = DF.loc[:, ~DF.columns.str.contains('^Unnamed')]\n",
    "DF['IC50 (nM)'] = DF['IC50 (nM)'].str.lstrip('<>').astype(float)\n",
    "\n",
    "\n",
    "DF = DF.loc[:, ~DF.columns.str.contains('^Unnamed')]\n",
    "\n",
    "\n",
    "DF = DF[(DF['FC'] == 0)] #& (DF['IC50 (nM)'] <= 1)]\n",
    "y = DF['IC50 (nM)']\n",
    "\n",
    "\n",
    "DF['pIC50'] = pIC50(DF)  # New column\n",
    "y = DF['pIC50']  # <-- Now using correct column\n",
    "X = DF.drop(columns=[\"ChEMBL ID\", \"FC\", 'IC50 (nM)', \"Smiles\", \"pIC50\"])  # Drop old IC50 and new pIC50\n",
    "\n",
    "# Scale the data\n",
    "# Scale X and y properly\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = StandardScaler()  # Changed to StandardScaler\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n"
   ],
   "id": "c2563e98664ef0f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "DF.head()",
   "id": "3e83da6e2b83593c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# joblib.dump(scaler_X, f\"scaler_X_{chembel_id}_SV.pkl\")\n",
    "\n",
    "# joblib.dump(scaler_y, f\"scaler_y_{chembel_id}_SV.pkl\")\n",
    "\n",
    "# K-Fold Cross-Validation setup\n",
    "n_folds = 5\n",
    "num_epochs = 200\n",
    "\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Store metrics across folds\n",
    "\n",
    "all_train_losses = []\n",
    "\n",
    "all_val_losses = []\n",
    "\n",
    "all_r2_scores = []\n",
    "\n",
    "all_mse_scores = []\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the neural network model\n",
    "\n",
    "class MultiOutputRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim,\n",
    "\n",
    "                 num_hidden_layers, dropout_rate):\n",
    "\n",
    "        super(MultiOutputRegressor, self).__init__()\n",
    "\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "\n",
    "            layers += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "\n",
    "                       nn.Dropout(dropout_rate)]\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_folds}\")\n",
    "\n",
    "    # Split data into training and validation sets for the current fold\n",
    "    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train, y_val = y_scaled[train_idx], y_scaled[val_idx]\n",
    "\n",
    "    # Print sizes of training and validation sets\n",
    "    print(f\"Training set size: {len(train_idx)}\")\n",
    "    print(f\"Validation set size: {len(val_idx)}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Convert to tensors\n",
    "\n",
    "    X_train_tensor = torch.from_numpy(X_train).float()\n",
    "\n",
    "    y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "    X_val_tensor = torch.from_numpy(X_val).float()\n",
    "\n",
    "    y_val_tensor = torch.from_numpy(y_val).float()\n",
    "\n",
    "\n",
    "\n",
    "    # Model, loss, optimizer, and scheduler settings\n",
    "\n",
    "    input_dim = X_train_tensor.shape[1]\n",
    "\n",
    "    hidden_dim = 256\n",
    "\n",
    "    num_hidden_layers = 4\n",
    "\n",
    "    output_dim = y_train_tensor.shape[1]\n",
    "\n",
    "    dropout_rate = 0.0\n",
    "\n",
    "\n",
    "\n",
    "    model = MultiOutputRegressor(input_dim, hidden_dim, output_dim, num_hidden_layers, dropout_rate)\n",
    "\n",
    "    criterion = nn.SmoothL1Loss(beta=1.0)  # Huber loss\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0041, weight_decay=1e-6)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "\n",
    "\n",
    "    # Prepare DataLoader\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop for the current fold\n",
    "\n",
    "    train_losses, val_losses, r2_scores, mse_scores = [], [], [], []\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        epoch_train_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "        train_losses.append(epoch_train_loss / len(train_loader))\n",
    "\n",
    "\n",
    "\n",
    "        # Validation\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        epoch_val_loss = 0\n",
    "\n",
    "        y_pred_val_all, y_val_all = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for X_batch, y_batch in val_loader:\n",
    "\n",
    "                y_pred = model(X_batch)\n",
    "\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "\n",
    "                epoch_val_loss += loss.item()\n",
    "\n",
    "                y_pred_val_all.append(y_pred)\n",
    "\n",
    "                y_val_all.append(y_batch)\n",
    "\n",
    "        val_losses.append(epoch_val_loss / len(val_loader))\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate R2 score\n",
    "\n",
    "        y_pred_val_all = torch.cat(y_pred_val_all, dim=0).cpu().numpy()\n",
    "\n",
    "        y_val_all = torch.cat(y_val_all, dim=0).cpu().numpy()\n",
    "\n",
    "        r2 = r2_score(y_val_all, y_pred_val_all, multioutput='variance_weighted')\n",
    "\n",
    "        MSE = mean_squared_error(y_val_all, y_pred_val_all)\n",
    "\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "        mse_scores.append(MSE)\n",
    "\n",
    "\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, R2: {r2:.4f}\")\n",
    "\n"
   ],
   "id": "2c8104fde5fec9bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "    # Store metrics for this fold\n",
    "\n",
    "    all_train_losses.append(train_losses)\n",
    "\n",
    "    all_val_losses.append(val_losses)\n",
    "\n",
    "    all_r2_scores.append(r2_scores)\n",
    "\n",
    "    all_mse_scores.append(mse_scores)\n",
    "\n",
    "\n",
    "\n",
    "# Aggregate metrics across folds\n",
    "\n",
    "average_train_losses = np.mean(all_train_losses, axis=0)\n",
    "\n",
    "average_val_losses = np.mean(all_val_losses, axis=0)\n",
    "\n",
    "average_r2_scores = np.mean(all_r2_scores, axis=0)\n",
    "\n",
    "avarage_mse_scores = np.mean(all_mse_scores, axis=0)\n",
    "\n",
    "print(f'final r2: {average_r2_scores[-1]} +/- {np.std(all_r2_scores, axis=0)[-1]}')\n",
    "\n",
    "print(f'final mse: {avarage_mse_scores[-1]} +/- {np.std(all_mse_scores, axis=0)[-1]}')\n",
    "\n",
    "# Save final metrics\n",
    "\n",
    "loss_data = pd.DataFrame({\n",
    "\n",
    "    'Epoch': range(1, num_epochs + 1),\n",
    "\n",
    "    'Avg_Train_Loss': average_train_losses,\n",
    "\n",
    "    'Avg_Val_Loss': average_val_losses,\n",
    "\n",
    "    'Avg_R2_Score': average_r2_scores,\n",
    "\n",
    "    'Avg_MSE': avarage_mse_scores,\n",
    "\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training completed. Exporting model for visualization...\")\n",
    "\n"
   ],
   "id": "57f1de9c32c4b3a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Save the final model (you might want to save the best model based on validation scores)\n",
    "\n",
    "# torch.save(model.state_dict(), f\"best_model_{chembel_id}_SV.pth\")\n",
    "\n",
    "# print(f\"Model saved as 'best_model_{chembel_id}_SV.pth'.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Inference on the entire dataset\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    X_tensor = torch.from_numpy(X_scaled).float()\n",
    "\n",
    "    y_pred = model(X_tensor)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate residuals\n",
    "\n",
    "y_pred_all = y_pred.cpu().numpy()\n",
    "\n",
    "y_actual_all = y_scaled  # use scaled y for consistency\n",
    "\n",
    "y_pred_test_all = scaler_y.inverse_transform(y_pred_all)  # Inverse scale predictions\n",
    "\n",
    "y_test_all = scaler_y.inverse_transform(y_actual_all)  # Inverse scale actual values\n",
    "\n",
    "residuals = (y_test_all - y_pred_test_all)  # Calculate residuals\n",
    "\n",
    "\n",
    "\n",
    "# Compute final RÂ² and MSE\n",
    "\n",
    "final_r2 = r2_score(y_test_all, y_pred_test_all, multioutput='variance_weighted')\n",
    "\n",
    "variance_y = np.var(y_test_all)\n",
    "\n",
    "MSE = mean_squared_error(y_actual_all, y_pred_all)\n",
    "\n",
    "NMSE = MSE / variance_y\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Final R2 Score: {final_r2:.4f}\")\n",
    "\n",
    "print(f\"MSE: {MSE:.4f}\")\n",
    "\n",
    "print(f\"NMSE: {NMSE:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Save performance metrics to loss data\n",
    "\n",
    "loss_data['Final_R2'] = final_r2\n",
    "\n",
    "loss_data['MSE'] = MSE\n",
    "\n",
    "loss_data['NMSE'] = NMSE\n",
    "\n",
    "\n",
    "\n",
    "# Save the loss data including the final metrics\n",
    "\n",
    "# loss_data.to_csv(f\"loss_and_r2_data_{chembel_id}_SV.csv\", index=False)\n",
    "\n",
    "#\n",
    "\n",
    "# # Save predictions and actual values for further analysis\n",
    "\n",
    "# np.savez(f\"predictions_vs_actuals_{chembel_id}_SV.npz\",\n",
    "\n",
    "#          y_pred=y_pred_test_all,\n",
    "\n",
    "#          y_test=y_test_all,\n",
    "\n",
    "#          residuals=residuals)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Inference completed and results saved.\")\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# Global settings for consistent styling\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "\n",
    "mpl.rcParams['legend.fontsize'] = 18\n",
    "\n",
    "mpl.rcParams['axes.labelsize'] = 18\n",
    "\n",
    "mpl.rcParams['xtick.labelsize'] = 16\n",
    "\n",
    "mpl.rcParams['ytick.labelsize'] = 16\n",
    "\n",
    "\n",
    "\n",
    "def clean_plot(ax, legend=True):\n",
    "\n",
    "    \"\"\"Custom function to clean plots.\"\"\"\n",
    "\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    ax.spines['left'].set_linewidth(3)    # Y-axis\n",
    "\n",
    "    ax.spines['bottom'].set_linewidth(3)  # X-axis\n",
    "\n",
    "    if legend:\n",
    "\n",
    "        ax.legend(frameon=False, fontsize=16)\n",
    "\n",
    "\n",
    "\n",
    "# Function to format ticks to scale of 10^3\n",
    "\n",
    "def scale_to_thousands(x, pos):\n",
    "\n",
    "    return f'{x/1e3:.0f}'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "formatter = FuncFormatter(scale_to_thousands)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(y_test_all, y_pred_test_all, alpha=0.6, label='SV')\n",
    "\n",
    "plt.plot([y_test_all.min(), y_test_all.max()],\n",
    "\n",
    "         [y_test_all.min(), y_test_all.max()], 'r--', label='line of equality')\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Actual Values\")# (x $10^3$)\")\n",
    "\n",
    "plt.ylabel(\"Predicted Values\")# (x $10^3$)\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# plt.gca().xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# plt.gca().yaxis.set_major_formatter(formatter)\n",
    "\n",
    "clean_plot(plt.gca())\n",
    "\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"Actual_vs_Predicted_G12D_SV.png\", dpi=300)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predicted_values = {}\n",
    "\n",
    "fda_pred = pd.read_csv(\"/Users/user/PycharmProjects/Drug Design FInal/FINAL_GIT/Raw Files/FDA_Hyb_Features.csv\")\n",
    "\n",
    "chembl_id_column = fda_pred['ChEMBL ID']\n",
    "\n",
    "smiles_column = fda_pred['Smiles']\n",
    "\n",
    "\n",
    "\n",
    "# Process FDA features (drop non-feature columns)\n",
    "\n",
    "X_new = fda_pred.drop(columns=[\"FC\", \"Smiles\", \"ChEMBL ID\"],\n",
    "\n",
    "                      errors='ignore')\n",
    "\n",
    "# Ensure X_new has all columns in X_train and in the correct order\n",
    "\n",
    "number_of_missing_features = 0\n",
    "\n",
    "for col in X.columns:\n",
    "\n",
    "    if col not in X_new.columns:\n",
    "\n",
    "        number_of_missing_features += 1\n",
    "\n",
    "        X_new[col] = 0  # Add missing columns with zeros\n",
    "\n",
    "print(number_of_missing_features, \"missing features added to X_new\")\n",
    "\n",
    "X_new = X_new[X.columns]  # Reorder columns to match X_train\n",
    "\n",
    "# Scale new FDA data\n",
    "\n",
    "X_new_scaled = scaler_X.transform(X_new)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "\n",
    "X_new_tensor = torch.from_numpy(X_new_scaled).float()\n",
    "\n",
    "\n",
    "\n",
    "# Predict new values\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    y_new_pred = model(X_new_tensor)\n",
    "\n",
    "\n",
    "\n",
    "# Convert predictions back to original pIC50 scale\n",
    "\n",
    "y_new_pred_original = scaler_y.inverse_transform(\n",
    "\n",
    "    y_new_pred.numpy()).flatten()\n",
    "\n",
    "\n",
    "\n",
    "# Store predictions\n",
    "\n",
    "for chembl_id, predicted_value in zip(chembl_id_column,\n",
    "\n",
    "                                      y_new_pred_original):\n",
    "\n",
    "    if chembl_id not in predicted_values:\n",
    "\n",
    "        predicted_values[chembl_id] = []\n",
    "\n",
    "    predicted_values[chembl_id].append(predicted_value)\n",
    "\n",
    "\n",
    "\n",
    "# print(predicted_values)\n",
    "\n",
    "\n",
    "\n",
    "sorted_values = sorted(predicted_values.items(), key=lambda x: x[1])\n",
    "\n",
    "molecules_df = pd.DataFrame(sorted_values[0:11],\n",
    "\n",
    "                            columns=['chembl_id', 'Predicted_Value'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "molecules_df.to_csv('NN_molecules_Newfeatures_G12D(1).csv')"
   ],
   "id": "97cd27cb2e814563"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
