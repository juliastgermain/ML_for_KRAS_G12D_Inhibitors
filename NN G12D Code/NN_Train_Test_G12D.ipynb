{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-28T14:45:41.068887Z",
     "start_time": "2025-02-28T14:45:35.817947Z"
    }
   },
   "source": [
    "#import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sympy.stats.sampling.sample_numpy import numpy\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "#from core.model import MultiOutputRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "#from core.visualize import export_and_visualize_model\n",
    "import joblib"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T14:49:53.741019Z",
     "start_time": "2025-02-28T14:48:44.611763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    DF = pd.read_csv(\"merged_features_IC50_g12d.csv\", index_col=False)\n",
    "    DF = DF.dropna()\n",
    "    DF = DF.loc[:, ~DF.columns.str.contains('^Unnamed')]\n",
    "    DF['IC50 (nM)'] = pd.to_numeric(DF['IC50 (nM)'], errors='coerce')\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = DF['IC50 (nM)'].quantile(0.25)\n",
    "    Q3 = DF['IC50 (nM)'].quantile(0.75)\n",
    "\n",
    "    # Calculate IQR\n",
    "    IQR = Q3 - Q1\n",
    "    # Relax the conditions by increasing the multiplier\n",
    "    multiplier = 0.6  # You can change this to a value that suits your needs\n",
    "\n",
    "    # Remove outliers using the relaxed IQR method\n",
    "    DF = DF[(DF['IC50 (nM)'] >= (Q1 - multiplier * IQR)) & (DF['IC50 (nM)'] <= (Q3 + multiplier * IQR))]\n",
    "    DF = DF.reset_index(drop=True)\n",
    "\n",
    "    # Filter and sample data\n",
    "    DF = DF[DF['FC'] == 0]\n",
    "\n",
    "    DF = DF.sample(frac=10, replace=True, random_state=42)  # Shuffle data\n",
    "\n",
    "    y = DF[[\"IC50 (nM)\"]] #  \"IC50 (nM)\"\n",
    "    X = DF.drop(columns=[\"FC\", \"IC50 (nM)\", \"Smiles\", \"ChEMBL ID\"])\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler_X = StandardScaler()\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "    scaler_y = StandardScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "    y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "\n",
    "    #joblib.dump(scaler_X, f\"saved_models/scaler_X_{chembel_id}_SV.pkl\")\n",
    "    #joblib.dump(scaler_y, f\"saved_models/scaler_y_{chembel_id}_SV.pkl\")\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.from_numpy(X_train_scaled).float()\n",
    "    X_test_tensor = torch.from_numpy(X_test_scaled).float()\n",
    "    y_train_tensor = torch.from_numpy(y_train_scaled).float()\n",
    "    y_test_tensor = torch.from_numpy(y_test_scaled).float()\n",
    "\n",
    "\n",
    "    # Model, loss, optimizer, and scheduler\n",
    "    input_dim = X_train_tensor.shape[1]\n",
    "    hidden_dim = 104  # Increased hidden dimension\n",
    "    num_hidden_layers = 4\n",
    "    output_dim = y_train_tensor.shape[1]\n",
    "    dropout_rate = 0.0\n",
    "\n",
    "\n",
    "    class MultiOutputNN(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, output_dim, num_hidden_layers, dropout_rate):\n",
    "            super(MultiOutputNN, self).__init__()\n",
    "            layers = []\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            for _ in range(num_hidden_layers - 1):\n",
    "                layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "            self.model = nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "\n",
    "    # Now create the model\n",
    "    model = MultiOutputNN(input_dim, hidden_dim, output_dim, num_hidden_layers, dropout_rate)\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()  # Huber loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0041, weight_decay=1e-6)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "    # Prepare DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    train_losses, test_losses, r2_scores = [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "        train_losses.append(epoch_train_loss / len(train_loader))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_test_loss = 0\n",
    "        y_pred_test_all, y_test_all = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                y_pred = model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                epoch_test_loss += loss.item()\n",
    "                y_pred_test_all.append(y_pred)\n",
    "                y_test_all.append(y_batch)\n",
    "        test_losses.append(epoch_test_loss / len(test_loader))\n",
    "\n",
    "        # Calculate R2 score\n",
    "        y_pred_test_all = torch.cat(y_pred_test_all, dim=0).cpu().numpy()\n",
    "        y_test_all = torch.cat(y_test_all, dim=0).cpu().numpy()\n",
    "        r2 = r2_score(y_test_all, y_pred_test_all, multioutput='variance_weighted')\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}, R2: {r2:.4f}\")"
   ],
   "id": "dec4044e770ff4e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.2517, Test Loss: 0.2023, R2: 0.3319\n",
      "Epoch 2/100, Train Loss: 0.2023, Test Loss: 0.1890, R2: 0.4062\n",
      "Epoch 3/100, Train Loss: 0.1847, Test Loss: 0.1639, R2: 0.4758\n",
      "Epoch 4/100, Train Loss: 0.1608, Test Loss: 0.1447, R2: 0.5384\n",
      "Epoch 5/100, Train Loss: 0.1484, Test Loss: 0.1395, R2: 0.5577\n",
      "Epoch 6/100, Train Loss: 0.1360, Test Loss: 0.1288, R2: 0.5752\n",
      "Epoch 7/100, Train Loss: 0.1272, Test Loss: 0.1183, R2: 0.6214\n",
      "Epoch 8/100, Train Loss: 0.1212, Test Loss: 0.1203, R2: 0.6290\n",
      "Epoch 9/100, Train Loss: 0.1147, Test Loss: 0.1116, R2: 0.6351\n",
      "Epoch 10/100, Train Loss: 0.1115, Test Loss: 0.1044, R2: 0.6604\n",
      "Epoch 11/100, Train Loss: 0.1054, Test Loss: 0.1091, R2: 0.6694\n",
      "Epoch 12/100, Train Loss: 0.1081, Test Loss: 0.1083, R2: 0.6519\n",
      "Epoch 13/100, Train Loss: 0.1020, Test Loss: 0.0959, R2: 0.6957\n",
      "Epoch 14/100, Train Loss: 0.0973, Test Loss: 0.0919, R2: 0.7130\n",
      "Epoch 15/100, Train Loss: 0.0933, Test Loss: 0.0948, R2: 0.6903\n",
      "Epoch 16/100, Train Loss: 0.0923, Test Loss: 0.0896, R2: 0.7183\n",
      "Epoch 17/100, Train Loss: 0.0894, Test Loss: 0.0907, R2: 0.7011\n",
      "Epoch 18/100, Train Loss: 0.0881, Test Loss: 0.0876, R2: 0.7228\n",
      "Epoch 19/100, Train Loss: 0.0891, Test Loss: 0.0898, R2: 0.6922\n",
      "Epoch 20/100, Train Loss: 0.0877, Test Loss: 0.0870, R2: 0.7119\n",
      "Epoch 21/100, Train Loss: 0.0834, Test Loss: 0.0825, R2: 0.7381\n",
      "Epoch 22/100, Train Loss: 0.0805, Test Loss: 0.0852, R2: 0.7168\n",
      "Epoch 23/100, Train Loss: 0.0847, Test Loss: 0.0825, R2: 0.7354\n",
      "Epoch 24/100, Train Loss: 0.0816, Test Loss: 0.0837, R2: 0.7362\n",
      "Epoch 25/100, Train Loss: 0.0779, Test Loss: 0.0783, R2: 0.7524\n",
      "Epoch 26/100, Train Loss: 0.0757, Test Loss: 0.0784, R2: 0.7502\n",
      "Epoch 27/100, Train Loss: 0.0721, Test Loss: 0.0762, R2: 0.7559\n",
      "Epoch 28/100, Train Loss: 0.0753, Test Loss: 0.0737, R2: 0.7687\n",
      "Epoch 29/100, Train Loss: 0.0715, Test Loss: 0.0778, R2: 0.7580\n",
      "Epoch 30/100, Train Loss: 0.0735, Test Loss: 0.0768, R2: 0.7469\n",
      "Epoch 31/100, Train Loss: 0.0751, Test Loss: 0.0782, R2: 0.7461\n",
      "Epoch 32/100, Train Loss: 0.0711, Test Loss: 0.0691, R2: 0.7798\n",
      "Epoch 33/100, Train Loss: 0.0672, Test Loss: 0.0698, R2: 0.7771\n",
      "Epoch 34/100, Train Loss: 0.0652, Test Loss: 0.0651, R2: 0.7953\n",
      "Epoch 35/100, Train Loss: 0.0611, Test Loss: 0.0672, R2: 0.7872\n",
      "Epoch 36/100, Train Loss: 0.0613, Test Loss: 0.0627, R2: 0.8067\n",
      "Epoch 37/100, Train Loss: 0.0594, Test Loss: 0.0605, R2: 0.8177\n",
      "Epoch 38/100, Train Loss: 0.0577, Test Loss: 0.0589, R2: 0.8204\n",
      "Epoch 39/100, Train Loss: 0.0585, Test Loss: 0.0635, R2: 0.7993\n",
      "Epoch 40/100, Train Loss: 0.0624, Test Loss: 0.0618, R2: 0.7982\n",
      "Epoch 41/100, Train Loss: 0.0569, Test Loss: 0.0600, R2: 0.8050\n",
      "Epoch 42/100, Train Loss: 0.0599, Test Loss: 0.0597, R2: 0.8151\n",
      "Epoch 43/100, Train Loss: 0.0554, Test Loss: 0.0574, R2: 0.8150\n",
      "Epoch 44/100, Train Loss: 0.0553, Test Loss: 0.0606, R2: 0.8121\n",
      "Epoch 45/100, Train Loss: 0.0532, Test Loss: 0.0552, R2: 0.8276\n",
      "Epoch 46/100, Train Loss: 0.0519, Test Loss: 0.0554, R2: 0.8233\n",
      "Epoch 47/100, Train Loss: 0.0515, Test Loss: 0.0517, R2: 0.8331\n",
      "Epoch 48/100, Train Loss: 0.0524, Test Loss: 0.0528, R2: 0.8366\n",
      "Epoch 49/100, Train Loss: 0.0504, Test Loss: 0.0518, R2: 0.8357\n",
      "Epoch 50/100, Train Loss: 0.0473, Test Loss: 0.0530, R2: 0.8291\n",
      "Epoch 51/100, Train Loss: 0.0443, Test Loss: 0.0473, R2: 0.8453\n",
      "Epoch 52/100, Train Loss: 0.0417, Test Loss: 0.0465, R2: 0.8492\n",
      "Epoch 53/100, Train Loss: 0.0401, Test Loss: 0.0448, R2: 0.8557\n",
      "Epoch 54/100, Train Loss: 0.0393, Test Loss: 0.0438, R2: 0.8518\n",
      "Epoch 55/100, Train Loss: 0.0387, Test Loss: 0.0428, R2: 0.8601\n",
      "Epoch 56/100, Train Loss: 0.0376, Test Loss: 0.0428, R2: 0.8568\n",
      "Epoch 57/100, Train Loss: 0.0386, Test Loss: 0.0425, R2: 0.8630\n",
      "Epoch 58/100, Train Loss: 0.0382, Test Loss: 0.0445, R2: 0.8556\n",
      "Epoch 59/100, Train Loss: 0.0378, Test Loss: 0.0435, R2: 0.8594\n",
      "Epoch 60/100, Train Loss: 0.0381, Test Loss: 0.0441, R2: 0.8553\n",
      "Epoch 61/100, Train Loss: 0.0376, Test Loss: 0.0425, R2: 0.8606\n",
      "Epoch 62/100, Train Loss: 0.0369, Test Loss: 0.0429, R2: 0.8577\n",
      "Epoch 63/100, Train Loss: 0.0355, Test Loss: 0.0441, R2: 0.8529\n",
      "Epoch 64/100, Train Loss: 0.0351, Test Loss: 0.0398, R2: 0.8744\n",
      "Epoch 65/100, Train Loss: 0.0339, Test Loss: 0.0400, R2: 0.8744\n",
      "Epoch 66/100, Train Loss: 0.0346, Test Loss: 0.0443, R2: 0.8532\n",
      "Epoch 67/100, Train Loss: 0.0343, Test Loss: 0.0400, R2: 0.8703\n",
      "Epoch 68/100, Train Loss: 0.0327, Test Loss: 0.0384, R2: 0.8798\n",
      "Epoch 69/100, Train Loss: 0.0329, Test Loss: 0.0380, R2: 0.8832\n",
      "Epoch 70/100, Train Loss: 0.0336, Test Loss: 0.0404, R2: 0.8734\n",
      "Epoch 71/100, Train Loss: 0.0343, Test Loss: 0.0431, R2: 0.8674\n",
      "Epoch 72/100, Train Loss: 0.0344, Test Loss: 0.0400, R2: 0.8707\n",
      "Epoch 73/100, Train Loss: 0.0316, Test Loss: 0.0403, R2: 0.8731\n",
      "Epoch 74/100, Train Loss: 0.0350, Test Loss: 0.0343, R2: 0.8941\n",
      "Epoch 75/100, Train Loss: 0.0340, Test Loss: 0.0385, R2: 0.8713\n",
      "Epoch 76/100, Train Loss: 0.0323, Test Loss: 0.0367, R2: 0.8802\n",
      "Epoch 77/100, Train Loss: 0.0291, Test Loss: 0.0359, R2: 0.8898\n",
      "Epoch 78/100, Train Loss: 0.0279, Test Loss: 0.0327, R2: 0.8973\n",
      "Epoch 79/100, Train Loss: 0.0303, Test Loss: 0.0385, R2: 0.8754\n",
      "Epoch 80/100, Train Loss: 0.0297, Test Loss: 0.0343, R2: 0.8895\n",
      "Epoch 81/100, Train Loss: 0.0316, Test Loss: 0.0332, R2: 0.8934\n",
      "Epoch 82/100, Train Loss: 0.0291, Test Loss: 0.0347, R2: 0.8845\n",
      "Epoch 83/100, Train Loss: 0.0280, Test Loss: 0.0329, R2: 0.8947\n",
      "Epoch 84/100, Train Loss: 0.0310, Test Loss: 0.0408, R2: 0.8667\n",
      "Epoch 85/100, Train Loss: 0.0357, Test Loss: 0.0414, R2: 0.8626\n",
      "Epoch 86/100, Train Loss: 0.0316, Test Loss: 0.0330, R2: 0.8963\n",
      "Epoch 87/100, Train Loss: 0.0264, Test Loss: 0.0338, R2: 0.8895\n",
      "Epoch 88/100, Train Loss: 0.0247, Test Loss: 0.0315, R2: 0.8990\n",
      "Epoch 89/100, Train Loss: 0.0226, Test Loss: 0.0311, R2: 0.8944\n",
      "Epoch 90/100, Train Loss: 0.0238, Test Loss: 0.0288, R2: 0.9059\n",
      "Epoch 91/100, Train Loss: 0.0246, Test Loss: 0.0307, R2: 0.8947\n",
      "Epoch 92/100, Train Loss: 0.0265, Test Loss: 0.0305, R2: 0.8954\n",
      "Epoch 93/100, Train Loss: 0.0270, Test Loss: 0.0316, R2: 0.8927\n",
      "Epoch 94/100, Train Loss: 0.0291, Test Loss: 0.0313, R2: 0.8937\n",
      "Epoch 95/100, Train Loss: 0.0249, Test Loss: 0.0293, R2: 0.8981\n",
      "Epoch 96/100, Train Loss: 0.0239, Test Loss: 0.0275, R2: 0.9060\n",
      "Epoch 97/100, Train Loss: 0.0248, Test Loss: 0.0339, R2: 0.8828\n",
      "Epoch 98/100, Train Loss: 0.0261, Test Loss: 0.0308, R2: 0.8916\n",
      "Epoch 99/100, Train Loss: 0.0285, Test Loss: 0.0367, R2: 0.8695\n",
      "Epoch 100/100, Train Loss: 0.0278, Test Loss: 0.0315, R2: 0.8952\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T14:49:53.909234Z",
     "start_time": "2025-02-28T14:49:53.901151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate final R2\n",
    "final_r2 = r2_scores[-1]\n",
    "MAE = mean_squared_error(y_test_all, y_pred_test_all)\n",
    "print(f\"Final R2 Score: {final_r2:.4f}\")\n",
    "print(f\"MSE: {MAE:.4f}\")"
   ],
   "id": "c10f7c9d9307fe3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final R2 Score: 0.8952\n",
      "MSE: 0.1031\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4bc501bbff5fa2aa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
