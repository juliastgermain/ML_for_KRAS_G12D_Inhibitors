{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-28T14:46:54.116658Z",
     "start_time": "2025-02-28T14:46:51.608883Z"
    }
   },
   "source": [
    "#import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sympy.stats.sampling.sample_numpy import numpy\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "#from core.model import MultiOutputRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "#from core.visualize import export_and_visualize_model\n",
    "import joblib"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T14:50:08.270422Z",
     "start_time": "2025-02-28T14:49:42.914637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    DF = pd.read_csv(\"CHEMBL2189121test_Features.csv\", index_col=False)\n",
    "    DF = DF.dropna()\n",
    "    DF = DF.loc[:, ~DF.columns.str.contains('^Unnamed')]\n",
    "    DF['SV'] = pd.to_numeric(DF['SV'], errors='coerce')\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = DF['SV'].quantile(0.25)\n",
    "    Q3 = DF['SV'].quantile(0.75)\n",
    "\n",
    "    # Calculate IQR\n",
    "    IQR = Q3 - Q1\n",
    "    # Relax the conditions by increasing the multiplier\n",
    "    multiplier = 0.6  # You can change this to a value that suits your needs\n",
    "\n",
    "    # Remove outliers using the relaxed IQR method\n",
    "    DF = DF[(DF['SV'] >= (Q1 - multiplier * IQR)) & (DF['SV'] <= (Q3 + multiplier * IQR))]\n",
    "    DF = DF.reset_index(drop=True)\n",
    "\n",
    "    # Filter and sample data\n",
    "    DF = DF[DF['FC'] == 0]\n",
    "\n",
    "    DF = DF.sample(frac=10, replace=True, random_state=42)  # Shuffle data\n",
    "\n",
    "    y = DF[[\"SV\"]] #  \"IC50 (nM)\"\n",
    "    X = DF.drop(columns=[\"FC\", \"SV\", \"SMILES\"])\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler_X = StandardScaler()\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "    scaler_y = StandardScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "    y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "\n",
    "    #joblib.dump(scaler_X, f\"saved_models/scaler_X_{chembel_id}_SV.pkl\")\n",
    "    #joblib.dump(scaler_y, f\"saved_models/scaler_y_{chembel_id}_SV.pkl\")\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.from_numpy(X_train_scaled).float()\n",
    "    X_test_tensor = torch.from_numpy(X_test_scaled).float()\n",
    "    y_train_tensor = torch.from_numpy(y_train_scaled).float()\n",
    "    y_test_tensor = torch.from_numpy(y_test_scaled).float()\n",
    "\n",
    "\n",
    "    # Model, loss, optimizer, and scheduler\n",
    "    input_dim = X_train_tensor.shape[1]\n",
    "    hidden_dim = 104  # Increased hidden dimension\n",
    "    num_hidden_layers = 4\n",
    "    output_dim = y_train_tensor.shape[1]\n",
    "    dropout_rate = 0.0\n",
    "\n",
    "\n",
    "    class MultiOutputNN(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, output_dim, num_hidden_layers, dropout_rate):\n",
    "            super(MultiOutputNN, self).__init__()\n",
    "            layers = []\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            for _ in range(num_hidden_layers - 1):\n",
    "                layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "            self.model = nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "\n",
    "    # Now create the model\n",
    "    model = MultiOutputNN(input_dim, hidden_dim, output_dim, num_hidden_layers, dropout_rate)\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()  # Huber loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0041, weight_decay=1e-6)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "    # Prepare DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    train_losses, test_losses, r2_scores = [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "        train_losses.append(epoch_train_loss / len(train_loader))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_test_loss = 0\n",
    "        y_pred_test_all, y_test_all = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                y_pred = model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                epoch_test_loss += loss.item()\n",
    "                y_pred_test_all.append(y_pred)\n",
    "                y_test_all.append(y_batch)\n",
    "        test_losses.append(epoch_test_loss / len(test_loader))\n",
    "\n",
    "        # Calculate R2 score\n",
    "        y_pred_test_all = torch.cat(y_pred_test_all, dim=0).cpu().numpy()\n",
    "        y_test_all = torch.cat(y_test_all, dim=0).cpu().numpy()\n",
    "        r2 = r2_score(y_test_all, y_pred_test_all, multioutput='variance_weighted')\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}, R2: {r2:.4f}\")"
   ],
   "id": "271086f0dfb8571f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.3125, Test Loss: 0.2663, R2: 0.1183\n",
      "Epoch 2/100, Train Loss: 0.2757, Test Loss: 0.2243, R2: 0.2760\n",
      "Epoch 3/100, Train Loss: 0.2288, Test Loss: 0.2027, R2: 0.4448\n",
      "Epoch 4/100, Train Loss: 0.2053, Test Loss: 0.1578, R2: 0.5146\n",
      "Epoch 5/100, Train Loss: 0.1825, Test Loss: 0.1479, R2: 0.5412\n",
      "Epoch 6/100, Train Loss: 0.1607, Test Loss: 0.1239, R2: 0.6322\n",
      "Epoch 7/100, Train Loss: 0.1394, Test Loss: 0.1154, R2: 0.6791\n",
      "Epoch 8/100, Train Loss: 0.1239, Test Loss: 0.1082, R2: 0.6946\n",
      "Epoch 9/100, Train Loss: 0.1074, Test Loss: 0.1006, R2: 0.7132\n",
      "Epoch 10/100, Train Loss: 0.1004, Test Loss: 0.0937, R2: 0.7328\n",
      "Epoch 11/100, Train Loss: 0.0947, Test Loss: 0.0947, R2: 0.7162\n",
      "Epoch 12/100, Train Loss: 0.1044, Test Loss: 0.0989, R2: 0.7132\n",
      "Epoch 13/100, Train Loss: 0.1063, Test Loss: 0.0993, R2: 0.7132\n",
      "Epoch 14/100, Train Loss: 0.1033, Test Loss: 0.0891, R2: 0.7495\n",
      "Epoch 15/100, Train Loss: 0.0933, Test Loss: 0.0898, R2: 0.7213\n",
      "Epoch 16/100, Train Loss: 0.0930, Test Loss: 0.0851, R2: 0.7610\n",
      "Epoch 17/100, Train Loss: 0.0887, Test Loss: 0.0831, R2: 0.7486\n",
      "Epoch 18/100, Train Loss: 0.0837, Test Loss: 0.0811, R2: 0.7608\n",
      "Epoch 19/100, Train Loss: 0.0842, Test Loss: 0.0820, R2: 0.7698\n",
      "Epoch 20/100, Train Loss: 0.0838, Test Loss: 0.0814, R2: 0.7708\n",
      "Epoch 21/100, Train Loss: 0.0783, Test Loss: 0.0802, R2: 0.7724\n",
      "Epoch 22/100, Train Loss: 0.0811, Test Loss: 0.0792, R2: 0.7759\n",
      "Epoch 23/100, Train Loss: 0.0776, Test Loss: 0.0726, R2: 0.7926\n",
      "Epoch 24/100, Train Loss: 0.0747, Test Loss: 0.0748, R2: 0.7935\n",
      "Epoch 25/100, Train Loss: 0.0739, Test Loss: 0.0730, R2: 0.7920\n",
      "Epoch 26/100, Train Loss: 0.0737, Test Loss: 0.0775, R2: 0.7550\n",
      "Epoch 27/100, Train Loss: 0.0770, Test Loss: 0.0770, R2: 0.7850\n",
      "Epoch 28/100, Train Loss: 0.0750, Test Loss: 0.0734, R2: 0.7769\n",
      "Epoch 29/100, Train Loss: 0.0741, Test Loss: 0.0798, R2: 0.7503\n",
      "Epoch 30/100, Train Loss: 0.0749, Test Loss: 0.0845, R2: 0.7314\n",
      "Epoch 31/100, Train Loss: 0.0747, Test Loss: 0.0722, R2: 0.7990\n",
      "Epoch 32/100, Train Loss: 0.0708, Test Loss: 0.0736, R2: 0.7928\n",
      "Epoch 33/100, Train Loss: 0.0718, Test Loss: 0.0781, R2: 0.7664\n",
      "Epoch 34/100, Train Loss: 0.0742, Test Loss: 0.0823, R2: 0.7326\n",
      "Epoch 35/100, Train Loss: 0.0715, Test Loss: 0.0690, R2: 0.8039\n",
      "Epoch 36/100, Train Loss: 0.0685, Test Loss: 0.0689, R2: 0.8047\n",
      "Epoch 37/100, Train Loss: 0.0665, Test Loss: 0.0665, R2: 0.8150\n",
      "Epoch 38/100, Train Loss: 0.0703, Test Loss: 0.0838, R2: 0.7490\n",
      "Epoch 39/100, Train Loss: 0.0817, Test Loss: 0.0796, R2: 0.7914\n",
      "Epoch 40/100, Train Loss: 0.0760, Test Loss: 0.0745, R2: 0.7784\n",
      "Epoch 41/100, Train Loss: 0.0733, Test Loss: 0.0663, R2: 0.8109\n",
      "Epoch 42/100, Train Loss: 0.0672, Test Loss: 0.0653, R2: 0.8155\n",
      "Epoch 43/100, Train Loss: 0.0638, Test Loss: 0.0643, R2: 0.8095\n",
      "Epoch 44/100, Train Loss: 0.0657, Test Loss: 0.0694, R2: 0.7957\n",
      "Epoch 45/100, Train Loss: 0.0620, Test Loss: 0.0596, R2: 0.8340\n",
      "Epoch 46/100, Train Loss: 0.0662, Test Loss: 0.0658, R2: 0.8082\n",
      "Epoch 47/100, Train Loss: 0.0677, Test Loss: 0.0647, R2: 0.8044\n",
      "Epoch 48/100, Train Loss: 0.0653, Test Loss: 0.0683, R2: 0.8053\n",
      "Epoch 49/100, Train Loss: 0.0681, Test Loss: 0.0679, R2: 0.8046\n",
      "Epoch 50/100, Train Loss: 0.0693, Test Loss: 0.0762, R2: 0.7768\n",
      "Epoch 51/100, Train Loss: 0.0718, Test Loss: 0.0684, R2: 0.8149\n",
      "Epoch 52/100, Train Loss: 0.0663, Test Loss: 0.0622, R2: 0.8268\n",
      "Epoch 53/100, Train Loss: 0.0616, Test Loss: 0.0579, R2: 0.8379\n",
      "Epoch 54/100, Train Loss: 0.0585, Test Loss: 0.0566, R2: 0.8416\n",
      "Epoch 55/100, Train Loss: 0.0576, Test Loss: 0.0553, R2: 0.8449\n",
      "Epoch 56/100, Train Loss: 0.0566, Test Loss: 0.0560, R2: 0.8478\n",
      "Epoch 57/100, Train Loss: 0.0534, Test Loss: 0.0549, R2: 0.8464\n",
      "Epoch 58/100, Train Loss: 0.0546, Test Loss: 0.0514, R2: 0.8517\n",
      "Epoch 59/100, Train Loss: 0.0597, Test Loss: 0.0650, R2: 0.8111\n",
      "Epoch 60/100, Train Loss: 0.0562, Test Loss: 0.0511, R2: 0.8550\n",
      "Epoch 61/100, Train Loss: 0.0543, Test Loss: 0.0504, R2: 0.8556\n",
      "Epoch 62/100, Train Loss: 0.0508, Test Loss: 0.0501, R2: 0.8587\n",
      "Epoch 63/100, Train Loss: 0.0511, Test Loss: 0.0572, R2: 0.8416\n",
      "Epoch 64/100, Train Loss: 0.0531, Test Loss: 0.0558, R2: 0.8503\n",
      "Epoch 65/100, Train Loss: 0.0534, Test Loss: 0.0519, R2: 0.8522\n",
      "Epoch 66/100, Train Loss: 0.0501, Test Loss: 0.0516, R2: 0.8501\n",
      "Epoch 67/100, Train Loss: 0.0510, Test Loss: 0.0549, R2: 0.8376\n",
      "Epoch 68/100, Train Loss: 0.0618, Test Loss: 0.0642, R2: 0.8114\n",
      "Epoch 69/100, Train Loss: 0.0605, Test Loss: 0.0535, R2: 0.8437\n",
      "Epoch 70/100, Train Loss: 0.0514, Test Loss: 0.0556, R2: 0.8450\n",
      "Epoch 71/100, Train Loss: 0.0528, Test Loss: 0.0492, R2: 0.8564\n",
      "Epoch 72/100, Train Loss: 0.0514, Test Loss: 0.0484, R2: 0.8616\n",
      "Epoch 73/100, Train Loss: 0.0500, Test Loss: 0.0485, R2: 0.8639\n",
      "Epoch 74/100, Train Loss: 0.0475, Test Loss: 0.0471, R2: 0.8662\n",
      "Epoch 75/100, Train Loss: 0.0478, Test Loss: 0.0475, R2: 0.8629\n",
      "Epoch 76/100, Train Loss: 0.0532, Test Loss: 0.0555, R2: 0.8398\n",
      "Epoch 77/100, Train Loss: 0.0592, Test Loss: 0.0611, R2: 0.8105\n",
      "Epoch 78/100, Train Loss: 0.0601, Test Loss: 0.0571, R2: 0.8363\n",
      "Epoch 79/100, Train Loss: 0.0548, Test Loss: 0.0491, R2: 0.8619\n",
      "Epoch 80/100, Train Loss: 0.0491, Test Loss: 0.0482, R2: 0.8571\n",
      "Epoch 81/100, Train Loss: 0.0472, Test Loss: 0.0467, R2: 0.8673\n",
      "Epoch 82/100, Train Loss: 0.0469, Test Loss: 0.0506, R2: 0.8577\n",
      "Epoch 83/100, Train Loss: 0.0481, Test Loss: 0.0535, R2: 0.8433\n",
      "Epoch 84/100, Train Loss: 0.0491, Test Loss: 0.0515, R2: 0.8509\n",
      "Epoch 85/100, Train Loss: 0.0527, Test Loss: 0.0464, R2: 0.8643\n",
      "Epoch 86/100, Train Loss: 0.0478, Test Loss: 0.0499, R2: 0.8636\n",
      "Epoch 87/100, Train Loss: 0.0487, Test Loss: 0.0477, R2: 0.8593\n",
      "Epoch 88/100, Train Loss: 0.0546, Test Loss: 0.0643, R2: 0.8110\n",
      "Epoch 89/100, Train Loss: 0.0598, Test Loss: 0.0477, R2: 0.8614\n",
      "Epoch 90/100, Train Loss: 0.0539, Test Loss: 0.0499, R2: 0.8601\n",
      "Epoch 91/100, Train Loss: 0.0507, Test Loss: 0.0557, R2: 0.8451\n",
      "Epoch 92/100, Train Loss: 0.0519, Test Loss: 0.0507, R2: 0.8545\n",
      "Epoch 93/100, Train Loss: 0.0573, Test Loss: 0.0487, R2: 0.8623\n",
      "Epoch 94/100, Train Loss: 0.0512, Test Loss: 0.0483, R2: 0.8606\n",
      "Epoch 95/100, Train Loss: 0.0497, Test Loss: 0.0527, R2: 0.8448\n",
      "Epoch 96/100, Train Loss: 0.0550, Test Loss: 0.0461, R2: 0.8642\n",
      "Epoch 97/100, Train Loss: 0.0460, Test Loss: 0.0466, R2: 0.8639\n",
      "Epoch 98/100, Train Loss: 0.0459, Test Loss: 0.0467, R2: 0.8680\n",
      "Epoch 99/100, Train Loss: 0.0455, Test Loss: 0.0486, R2: 0.8621\n",
      "Epoch 100/100, Train Loss: 0.0476, Test Loss: 0.0470, R2: 0.8682\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T14:50:10.048234Z",
     "start_time": "2025-02-28T14:50:10.041231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate final R2\n",
    "final_r2 = r2_scores[-1]\n",
    "MAE = mean_squared_error(y_test_all, y_pred_test_all)\n",
    "print(f\"Final R2 Score: {final_r2:.4f}\")\n",
    "print(f\"MSE: {MAE:.4f}\")"
   ],
   "id": "42efee733ef203f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final R2 Score: 0.8682\n",
      "MSE: 0.1231\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e5d8c51e174d232c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
