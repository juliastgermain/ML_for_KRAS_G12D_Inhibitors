{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T14:48:29.585761Z",
     "start_time": "2025-02-28T14:48:29.580695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sympy.stats.sampling.sample_numpy import numpy\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "#from core.model import MultiOutputRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "#from core.visualize import export_and_visualize_model\n",
    "import joblib"
   ],
   "id": "37aa26bcf2e8e78b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T14:49:09.520063Z",
     "start_time": "2025-02-28T14:48:29.847012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    DF = pd.read_csv(\"new_merged_features_IC50_g12c.csv\", index_col=False)\n",
    "    DF = DF.dropna()\n",
    "    DF = DF.loc[:, ~DF.columns.str.contains('^Unnamed')]\n",
    "    DF['IC50 (nM)'] = pd.to_numeric(DF['IC50 (nM)'], errors='coerce')\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = DF['IC50 (nM)'].quantile(0.25)\n",
    "    Q3 = DF['IC50 (nM)'].quantile(0.75)\n",
    "\n",
    "    # Calculate IQR\n",
    "    IQR = Q3 - Q1\n",
    "    # Relax the conditions by increasing the multiplier\n",
    "    multiplier = 0.6  # You can change this to a value that suits your needs\n",
    "\n",
    "    # Remove outliers using the relaxed IQR method\n",
    "    DF = DF[(DF['IC50 (nM)'] >= (Q1 - multiplier * IQR)) & (DF['IC50 (nM)'] <= (Q3 + multiplier * IQR))]\n",
    "    DF = DF.reset_index(drop=True)\n",
    "\n",
    "    # Filter and sample data\n",
    "    DF = DF[DF['FC'] == 0]\n",
    "\n",
    "    DF = DF.sample(frac=10, replace=True, random_state=42)  # Shuffle data\n",
    "\n",
    "    y = DF[[\"IC50 (nM)\"]] #  \"IC50 (nM)\"\n",
    "    X = DF.drop(columns=[\"FC\", \"IC50 (nM)\", \"Smiles\", \"ChEMBL ID\"])\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler_X = StandardScaler()\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "    scaler_y = StandardScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "    y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "\n",
    "    #joblib.dump(scaler_X, f\"saved_models/scaler_X_{chembel_id}_SV.pkl\")\n",
    "    #joblib.dump(scaler_y, f\"saved_models/scaler_y_{chembel_id}_SV.pkl\")\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.from_numpy(X_train_scaled).float()\n",
    "    X_test_tensor = torch.from_numpy(X_test_scaled).float()\n",
    "    y_train_tensor = torch.from_numpy(y_train_scaled).float()\n",
    "    y_test_tensor = torch.from_numpy(y_test_scaled).float()\n",
    "\n",
    "\n",
    "    # Model, loss, optimizer, and scheduler\n",
    "    input_dim = X_train_tensor.shape[1]\n",
    "    hidden_dim = 104  # Increased hidden dimension\n",
    "    num_hidden_layers = 4\n",
    "    output_dim = y_train_tensor.shape[1]\n",
    "    dropout_rate = 0.0\n",
    "\n",
    "\n",
    "    class MultiOutputNN(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, output_dim, num_hidden_layers, dropout_rate):\n",
    "            super(MultiOutputNN, self).__init__()\n",
    "            layers = []\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            for _ in range(num_hidden_layers - 1):\n",
    "                layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "            self.model = nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "\n",
    "    # Now create the model\n",
    "    model = MultiOutputNN(input_dim, hidden_dim, output_dim, num_hidden_layers, dropout_rate)\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()  # Huber loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0041, weight_decay=1e-6)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "    # Prepare DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    train_losses, test_losses, r2_scores = [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "        train_losses.append(epoch_train_loss / len(train_loader))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_test_loss = 0\n",
    "        y_pred_test_all, y_test_all = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                y_pred = model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                epoch_test_loss += loss.item()\n",
    "                y_pred_test_all.append(y_pred)\n",
    "                y_test_all.append(y_batch)\n",
    "        test_losses.append(epoch_test_loss / len(test_loader))\n",
    "\n",
    "        # Calculate R2 score\n",
    "        y_pred_test_all = torch.cat(y_pred_test_all, dim=0).cpu().numpy()\n",
    "        y_test_all = torch.cat(y_test_all, dim=0).cpu().numpy()\n",
    "        r2 = r2_score(y_test_all, y_pred_test_all, multioutput='variance_weighted')\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}, R2: {r2:.4f}\")"
   ],
   "id": "4706e617fbf0a126",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.3031, Test Loss: 0.2185, R2: 0.4490\n",
      "Epoch 2/100, Train Loss: 0.2105, Test Loss: 0.1836, R2: 0.5766\n",
      "Epoch 3/100, Train Loss: 0.1612, Test Loss: 0.1488, R2: 0.6747\n",
      "Epoch 4/100, Train Loss: 0.1321, Test Loss: 0.1182, R2: 0.7312\n",
      "Epoch 5/100, Train Loss: 0.1173, Test Loss: 0.1027, R2: 0.7636\n",
      "Epoch 6/100, Train Loss: 0.1042, Test Loss: 0.1028, R2: 0.7844\n",
      "Epoch 7/100, Train Loss: 0.0958, Test Loss: 0.0915, R2: 0.8086\n",
      "Epoch 8/100, Train Loss: 0.0879, Test Loss: 0.0842, R2: 0.8218\n",
      "Epoch 9/100, Train Loss: 0.0829, Test Loss: 0.0824, R2: 0.8220\n",
      "Epoch 10/100, Train Loss: 0.0797, Test Loss: 0.0842, R2: 0.8317\n",
      "Epoch 11/100, Train Loss: 0.0787, Test Loss: 0.0987, R2: 0.8200\n",
      "Epoch 12/100, Train Loss: 0.0765, Test Loss: 0.0700, R2: 0.8316\n",
      "Epoch 13/100, Train Loss: 0.0766, Test Loss: 0.0681, R2: 0.8359\n",
      "Epoch 14/100, Train Loss: 0.0720, Test Loss: 0.0776, R2: 0.8327\n",
      "Epoch 15/100, Train Loss: 0.0698, Test Loss: 0.0670, R2: 0.8472\n",
      "Epoch 16/100, Train Loss: 0.0662, Test Loss: 0.0598, R2: 0.8494\n",
      "Epoch 17/100, Train Loss: 0.0649, Test Loss: 0.0536, R2: 0.8621\n",
      "Epoch 18/100, Train Loss: 0.0631, Test Loss: 0.0534, R2: 0.8668\n",
      "Epoch 19/100, Train Loss: 0.0641, Test Loss: 0.0696, R2: 0.8454\n",
      "Epoch 20/100, Train Loss: 0.0643, Test Loss: 0.0644, R2: 0.8647\n",
      "Epoch 21/100, Train Loss: 0.0613, Test Loss: 0.0497, R2: 0.8801\n",
      "Epoch 22/100, Train Loss: 0.0567, Test Loss: 0.0527, R2: 0.8762\n",
      "Epoch 23/100, Train Loss: 0.0568, Test Loss: 0.0465, R2: 0.8851\n",
      "Epoch 24/100, Train Loss: 0.0524, Test Loss: 0.0443, R2: 0.8871\n",
      "Epoch 25/100, Train Loss: 0.0520, Test Loss: 0.0453, R2: 0.8830\n",
      "Epoch 26/100, Train Loss: 0.0522, Test Loss: 0.0461, R2: 0.8896\n",
      "Epoch 27/100, Train Loss: 0.0514, Test Loss: 0.0498, R2: 0.8835\n",
      "Epoch 28/100, Train Loss: 0.0539, Test Loss: 0.0527, R2: 0.8847\n",
      "Epoch 29/100, Train Loss: 0.0520, Test Loss: 0.0442, R2: 0.8894\n",
      "Epoch 30/100, Train Loss: 0.0490, Test Loss: 0.0438, R2: 0.8981\n",
      "Epoch 31/100, Train Loss: 0.0475, Test Loss: 0.0363, R2: 0.9086\n",
      "Epoch 32/100, Train Loss: 0.0443, Test Loss: 0.0352, R2: 0.9111\n",
      "Epoch 33/100, Train Loss: 0.0428, Test Loss: 0.0389, R2: 0.9044\n",
      "Epoch 34/100, Train Loss: 0.0436, Test Loss: 0.0389, R2: 0.9059\n",
      "Epoch 35/100, Train Loss: 0.0451, Test Loss: 0.0422, R2: 0.8937\n",
      "Epoch 36/100, Train Loss: 0.0446, Test Loss: 0.0362, R2: 0.9091\n",
      "Epoch 37/100, Train Loss: 0.0425, Test Loss: 0.0401, R2: 0.9039\n",
      "Epoch 38/100, Train Loss: 0.0425, Test Loss: 0.0414, R2: 0.8944\n",
      "Epoch 39/100, Train Loss: 0.0438, Test Loss: 0.0388, R2: 0.9020\n",
      "Epoch 40/100, Train Loss: 0.0432, Test Loss: 0.0357, R2: 0.9095\n",
      "Epoch 41/100, Train Loss: 0.0397, Test Loss: 0.0328, R2: 0.9191\n",
      "Epoch 42/100, Train Loss: 0.0395, Test Loss: 0.0314, R2: 0.9218\n",
      "Epoch 43/100, Train Loss: 0.0415, Test Loss: 0.0327, R2: 0.9197\n",
      "Epoch 44/100, Train Loss: 0.0410, Test Loss: 0.0384, R2: 0.9072\n",
      "Epoch 45/100, Train Loss: 0.0434, Test Loss: 0.0338, R2: 0.9148\n",
      "Epoch 46/100, Train Loss: 0.0405, Test Loss: 0.0331, R2: 0.9186\n",
      "Epoch 47/100, Train Loss: 0.0376, Test Loss: 0.0285, R2: 0.9300\n",
      "Epoch 48/100, Train Loss: 0.0371, Test Loss: 0.0337, R2: 0.9222\n",
      "Epoch 49/100, Train Loss: 0.0387, Test Loss: 0.0330, R2: 0.9184\n",
      "Epoch 50/100, Train Loss: 0.0389, Test Loss: 0.0312, R2: 0.9260\n",
      "Epoch 51/100, Train Loss: 0.0344, Test Loss: 0.0307, R2: 0.9281\n",
      "Epoch 52/100, Train Loss: 0.0320, Test Loss: 0.0273, R2: 0.9353\n",
      "Epoch 53/100, Train Loss: 0.0299, Test Loss: 0.0246, R2: 0.9397\n",
      "Epoch 54/100, Train Loss: 0.0296, Test Loss: 0.0241, R2: 0.9411\n",
      "Epoch 55/100, Train Loss: 0.0283, Test Loss: 0.0238, R2: 0.9422\n",
      "Epoch 56/100, Train Loss: 0.0292, Test Loss: 0.0272, R2: 0.9333\n",
      "Epoch 57/100, Train Loss: 0.0300, Test Loss: 0.0250, R2: 0.9398\n",
      "Epoch 58/100, Train Loss: 0.0296, Test Loss: 0.0270, R2: 0.9362\n",
      "Epoch 59/100, Train Loss: 0.0286, Test Loss: 0.0242, R2: 0.9423\n",
      "Epoch 60/100, Train Loss: 0.0267, Test Loss: 0.0229, R2: 0.9432\n",
      "Epoch 61/100, Train Loss: 0.0268, Test Loss: 0.0219, R2: 0.9471\n",
      "Epoch 62/100, Train Loss: 0.0276, Test Loss: 0.0215, R2: 0.9471\n",
      "Epoch 63/100, Train Loss: 0.0279, Test Loss: 0.0226, R2: 0.9449\n",
      "Epoch 64/100, Train Loss: 0.0277, Test Loss: 0.0241, R2: 0.9416\n",
      "Epoch 65/100, Train Loss: 0.0270, Test Loss: 0.0234, R2: 0.9451\n",
      "Epoch 66/100, Train Loss: 0.0255, Test Loss: 0.0204, R2: 0.9499\n",
      "Epoch 67/100, Train Loss: 0.0251, Test Loss: 0.0236, R2: 0.9434\n",
      "Epoch 68/100, Train Loss: 0.0247, Test Loss: 0.0213, R2: 0.9499\n",
      "Epoch 69/100, Train Loss: 0.0238, Test Loss: 0.0216, R2: 0.9486\n",
      "Epoch 70/100, Train Loss: 0.0249, Test Loss: 0.0222, R2: 0.9457\n",
      "Epoch 71/100, Train Loss: 0.0243, Test Loss: 0.0194, R2: 0.9534\n",
      "Epoch 72/100, Train Loss: 0.0238, Test Loss: 0.0205, R2: 0.9512\n",
      "Epoch 73/100, Train Loss: 0.0229, Test Loss: 0.0197, R2: 0.9539\n",
      "Epoch 74/100, Train Loss: 0.0227, Test Loss: 0.0198, R2: 0.9524\n",
      "Epoch 75/100, Train Loss: 0.0230, Test Loss: 0.0200, R2: 0.9541\n",
      "Epoch 76/100, Train Loss: 0.0221, Test Loss: 0.0194, R2: 0.9529\n",
      "Epoch 77/100, Train Loss: 0.0224, Test Loss: 0.0193, R2: 0.9544\n",
      "Epoch 78/100, Train Loss: 0.0231, Test Loss: 0.0184, R2: 0.9543\n",
      "Epoch 79/100, Train Loss: 0.0225, Test Loss: 0.0184, R2: 0.9540\n",
      "Epoch 80/100, Train Loss: 0.0253, Test Loss: 0.0220, R2: 0.9484\n",
      "Epoch 81/100, Train Loss: 0.0234, Test Loss: 0.0202, R2: 0.9513\n",
      "Epoch 82/100, Train Loss: 0.0227, Test Loss: 0.0186, R2: 0.9536\n",
      "Epoch 83/100, Train Loss: 0.0210, Test Loss: 0.0188, R2: 0.9555\n",
      "Epoch 84/100, Train Loss: 0.0210, Test Loss: 0.0176, R2: 0.9580\n",
      "Epoch 85/100, Train Loss: 0.0235, Test Loss: 0.0176, R2: 0.9565\n",
      "Epoch 86/100, Train Loss: 0.0220, Test Loss: 0.0193, R2: 0.9537\n",
      "Epoch 87/100, Train Loss: 0.0231, Test Loss: 0.0177, R2: 0.9566\n",
      "Epoch 88/100, Train Loss: 0.0231, Test Loss: 0.0170, R2: 0.9574\n",
      "Epoch 89/100, Train Loss: 0.0207, Test Loss: 0.0173, R2: 0.9581\n",
      "Epoch 90/100, Train Loss: 0.0206, Test Loss: 0.0173, R2: 0.9584\n",
      "Epoch 91/100, Train Loss: 0.0214, Test Loss: 0.0178, R2: 0.9568\n",
      "Epoch 92/100, Train Loss: 0.0211, Test Loss: 0.0182, R2: 0.9584\n",
      "Epoch 93/100, Train Loss: 0.0189, Test Loss: 0.0160, R2: 0.9608\n",
      "Epoch 94/100, Train Loss: 0.0187, Test Loss: 0.0166, R2: 0.9596\n",
      "Epoch 95/100, Train Loss: 0.0186, Test Loss: 0.0152, R2: 0.9635\n",
      "Epoch 96/100, Train Loss: 0.0181, Test Loss: 0.0150, R2: 0.9639\n",
      "Epoch 97/100, Train Loss: 0.0182, Test Loss: 0.0157, R2: 0.9635\n",
      "Epoch 98/100, Train Loss: 0.0188, Test Loss: 0.0171, R2: 0.9581\n",
      "Epoch 99/100, Train Loss: 0.0184, Test Loss: 0.0159, R2: 0.9613\n",
      "Epoch 100/100, Train Loss: 0.0172, Test Loss: 0.0142, R2: 0.9660\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T14:49:09.547898Z",
     "start_time": "2025-02-28T14:49:09.539555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate final R2\n",
    "final_r2 = r2_scores[-1]\n",
    "MAE = mean_squared_error(y_test_all, y_pred_test_all)\n",
    "print(f\"Final R2 Score: {final_r2:.4f}\")\n",
    "print(f\"MSE: {MAE:.4f}\")"
   ],
   "id": "bca220d0d8d80e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final R2 Score: 0.9660\n",
      "MSE: 0.0342\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "23373a38f40c4fd2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
